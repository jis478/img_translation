
## Image to Image translation using StyleGAN V2


#### 1. Motivation 

 
Inspired by [Justin's work](https://www.justinpinkney.com/making-toonify/), I was wondering whether this technique could be applied to FFHQ -> Anime image translation. Although [Ukiyoe translation](https://www.justinpinkney.com/stylegan-network-blending/) looks very successful, it seems not to involve much of shape translation but rather to focus on texture translation.

As seen in the below, network blending can translate FFHQ images (generated by StyleGAN V2) into Danbooru anime images at 4x4 level, but adds too much "anime factors" ignoring the global structures of the original image. For me, it seems like that 16x16 level blending produces the best translation quality in terms of global structures (eg. pose, hair style..) but the overall translation quality degrades at this level. <br>

- Pre-trained model: StyleGAN v2 FFHQ model (256x256)
- Fine-tuned model: StyleGAN v2 Danbooru anime model (256x256) trained on 10,000 randomly sampled Danbooru images
- Low: FFHQ pre-trained, High: Danbooru anime fine-tuned

![Representative image](https://github.com/jis478/img_translation/blob/main/imgs/ffhq_to_anime.png)


#### 2. Optimization 
I thought this whole network blending technique is awesome but misses an optimization process so I've decided to add CycleGAN as a post-netowrk blending job. I was hoping to take advantage of CycleGAN's weakness in shape translation as enough shape translation has been done thanks to network blending, and what has to be focused on is texture translation which can be carried over from 4x4 level bleding images. In other words, I was expecting to generate images with 16x16 level global structures and 4x4 textures.

- (Domain A): Network blending images at 16x16
- (Domain B): Network blending images at 4x4
- CycleGAN from Domain A -> Domain B 


The below are some sample images. CycleGAN seems to work to some extent by producing fake images preserving the original images' global features. However, the result is still behind SOTA image to image translation models such as U-GAT-IT, but it did shed a light on using StyleGAN for image translation work involving shape translation. 

![Representative image](https://github.com/jis478/img_translation/blob/main/imgs/cyclegan_opt.png)


It's also interesting to compare with U-GAT-IT. U-GAT-IT focuses more on texture translation preserving global structures of the input image, but network blending seem to focus on brining more "anime features" (even sometimes ignores original global features)

![Representative image](https://github.com/jis478/img_translation/blob/main/imgs/ugatit.JPG)


#### 3. Further work
I will look into other GANs for optimization purpose, epspecially GANs known for seperating global and style features. Will post more results here soon :)






